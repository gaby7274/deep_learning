{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition is on other file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#   Model Definition\n",
    "###\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding, is it worth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encodings, according to a recent paper transformers can infer positional information for music generation. Could try no positional encoding vs sinusoidal positional encoding vs book positional encoding. Sinusoidal has a more dynamic approach than positional encoding, that has a fixed size. \n",
    "\n",
    "implementation 13 to 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Pilot\n",
    "## Key Differences\n",
    "#### Precomputation vs. On-the-fly Calculation:\n",
    "\n",
    "PositionalEncoding precomputes the positional encodings for a fixed maximum length (max_len) and stores them in self.P. This can be more efficient if the sequence lengths are known and fixed.\n",
    "SinusoidalPosEmbedding computes the positional encodings on-the-fly based on the input tensor x. This can be more flexible for varying sequence lengths.\n",
    "#### Usage:\n",
    "\n",
    "PositionalEncoding is typically used by adding the precomputed positional encodings to the input embeddings.\n",
    "SinusoidalPosEmbedding generates the positional encodings directly from the input tensor and can be used to concatenate or add to the input embeddings.\n",
    "#### Implementation Details:\n",
    "\n",
    "PositionalEncoding uses a fixed maximum length and creates a large tensor self.P to store the positional encodings.\n",
    "SinusoidalPosEmbedding calculates the positional encodings dynamically using the input tensor x and does not require a fixed maximum length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCKS ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In model_architecture.py lines from  59 -44 we define the attention block which will contain:\n",
    " - Multihead attention, where query key value will be distributed so each head can learn different features. \n",
    " One paper recommended 4 heads for music generation. \n",
    " - Causal masking: Basically masks the input that goes after the next-to-predict token, to prevent the model looking foward in thye sequences to predict a token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model_architecture.py lines from 99-160 we define or transformer blocks. These consists of:\n",
    "- one  multihead attention layer\n",
    "- followed by norm and dropout layers\n",
    "- a parameter that sets another multihead norm dropout if the block is classified as a decoder. This is to implement cross attention\n",
    "- later an mlp layer withy addition and norm/dropout\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MusicalEmbeder\n",
    "To process the embeddings we need to define a custom embedder that contains \n",
    "- Embedding layer, we give the number of token classes, (6) and embedsing dimension size\n",
    "- linear layer to extract embeddings/information from individual features\n",
    "- Concat features with token embeddings\n",
    "- Single vector to reduce in linear regression. Sort Cone to bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model_architecture.py lines from 165-212 we define our encoder blocks. These consists of:\n",
    "- an embedding layer,\n",
    "\n",
    "- followed by an optional positional encodding method, could be book based or sinusoidal positional encoding. \n",
    "- Then an N amount of transformer blocks. \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from 214-250, similar to encoder\n",
    "- positional embedding, with extra layers on transformer blocks to use cross attention\n",
    "- followed by a fully connected layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Encoder Decoder (ver1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablation studies of architectures. We will have an encoder decoder structure that is one encoder with two stacked encoder blocks, and two(and three) output blocks. These numbers were chosen based on the input (two type of tokens for input, three outputs). Later compare. Should be bad. Evaluation is gonna be token error rate. \n",
    "\n",
    "theversion is on 252 until 276, simple encoder decoder training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class encoder Decoder v2\n",
    "\n",
    "The best model out of this is going to be compared with another encoder decoder with two encoders and three decoders, one for each input and one of each output. Plan is to train encoder with embeddings for each type of tokens, then add the two encoded embeddings and feed it into the three decoders, each decoder for one voice. \n",
    "The problem is how will the output be managed / how will the model be lossed. Probably calculate loss by adding loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora fue, data preparation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##code taken from github  deepChoir, and modified accordingly.\n",
    "## QUantize score to have it all squared\n",
    "## Normalizesall notesand scores.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "# from config import *\n",
    "import music21 as m21\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# 'loader.py'\n",
    "EXTENSION = ['.musicxml', '.xml', '.mxl']\n",
    "DATASET_PATH = \"gaby_deeplearning/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/gabri/OneDrive/Documents/multihead_attention_music\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def quant_score(score):\n",
    "    \n",
    "    for element in score.flatten():\n",
    "        onset = np.ceil(element.offset/0.25)*0.25\n",
    "\n",
    "        if isinstance(element, m21.note.Note) or isinstance(element, m21.note.Rest) or isinstance(element, m21.chord.Chord):\n",
    "            offset = np.ceil((element.offset+element.quarterLength)/0.25)*0.25\n",
    "            element.quarterLength = offset - onset\n",
    "\n",
    "        element.offset = onset\n",
    "\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverse input dir, customimplementation of glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##gets the file names of each score. \n",
    "\n",
    "def get_filenames(input_dir):\n",
    "    \n",
    "    filenames = []\n",
    "\n",
    "    # Traverse the path\n",
    "    for dirpath, dirlist, filelist in os.walk(input_dir):\n",
    "        # Traverse the list of files\n",
    "        for this_file in filelist:\n",
    "            # Ensure that suffixes in the training set are valid\n",
    "            if input_dir==DATASET_PATH and os.path.splitext(this_file)[-1] not in EXTENSION:\n",
    "                continue\n",
    "            filename = os.path.join(dirpath, this_file)\n",
    "\n",
    "\n",
    "            score = m21.converter.parse(filename)\n",
    "            skippable=False\n",
    "            #   ## Added, if the score has a 3/4 time signature, we will skip it.\n",
    "            for part in score.parts:\n",
    "                if skippable:\n",
    "                    break\n",
    "                for element in part.flatten():\n",
    "                    \n",
    "                    if isinstance(element, m21.meter.TimeSignature):\n",
    "                        if element.numerator%3 == 0 or element.numerator==2:\n",
    "                    \n",
    "                            print('skipping 3/4 time signature')\n",
    "                            skippable=True\n",
    "                            break\n",
    "                        \n",
    "\n",
    "            if skippable:\n",
    "                continue\n",
    "\n",
    "            filenames.append(filename)\n",
    "\n",
    "\n",
    "    return filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize key signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# key signature to gap\n",
    "def ks2gap(ks):\n",
    "    \n",
    "    if isinstance(ks, m21.key.KeySignature):\n",
    "        ks = ks.asKey()\n",
    "        \n",
    "    try:\n",
    "        # Identify the tonic\n",
    "        # print('printing tonic')\n",
    "        # print(ks.tonic)\n",
    "        if ks.mode == 'major':\n",
    "            tonic = ks.tonic\n",
    "\n",
    "        else:\n",
    "            # print('tonic not major, ks')\n",
    "            tonic = ks.parallel.tonic\n",
    "            # print(tonic)\n",
    "    \n",
    "    except:\n",
    "        return m21.interval.Interval(0)\n",
    "\n",
    "    # Transpose score\n",
    "    gap = m21.interval.Interval(tonic, m21.pitch.Pitch('C'))\n",
    "\n",
    "    return gap.semitones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNused code? Each different key signature inside a scorewill be splitted into a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Split score by key signature\n",
    "\n",
    "def split_by_key(score):\n",
    "\n",
    "    scores = []\n",
    "    score_part = []\n",
    "    ks_list = []\n",
    "    ks = None\n",
    "    ts = m21.meter.TimeSignature('c')\n",
    "    pre_offset = 0\n",
    "\n",
    "    for element in score.flatten():\n",
    "\n",
    "        # If is key signature\n",
    "        if isinstance(element, m21.key.KeySignature) or isinstance(element, m21.key.Key):\n",
    "\n",
    "            # If is not the first key signature\n",
    "            if ks!=None:\n",
    "\n",
    "                scores.append(m21.stream.Stream(score_part))\n",
    "                ks = element\n",
    "                ks_list.append(ks)\n",
    "                pre_offset = ks.offset\n",
    "                ks.offset = 0\n",
    "                new_ts = m21.meter.TimeSignature(ts.ratioString)\n",
    "                score_part = [ks, new_ts]\n",
    "            \n",
    "            else:\n",
    "\n",
    "                ks = element\n",
    "                ks_list.append(ks)\n",
    "                score_part.append(ks)\n",
    "\n",
    "        # If is time signature\n",
    "        elif isinstance(element, m21.meter.TimeSignature):\n",
    "\n",
    "            element.offset -= pre_offset\n",
    "            ts = element\n",
    "            score_part.append(element)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            element.offset -= pre_offset\n",
    "            score_part.append(element)\n",
    "\n",
    "    scores.append(m21.stream.Stream(score_part))\n",
    "    if ks_list==[]:\n",
    "        ks_list = [m21.key.KeySignature(0)]\n",
    "        \n",
    "    gap_list = [ks2gap(ks) for ks in ks_list]\n",
    "\n",
    "    return scores, gap_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating beat strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Needs modification?? how to implement beats in remi?\n",
    "def beat_seq(ts):\n",
    "\n",
    "    # Read time signature\n",
    "    beatCount = ts.numerator\n",
    "    beatDuration = 4/ts.denominator\n",
    "\n",
    "    # Create beat sequence\n",
    "    beat_sequence = [0]*beatCount*int(beatDuration/0.25)\n",
    "    beat_sequence[0] += 1\n",
    "\n",
    "    # Check if the numerator is divisible by 3 or 2\n",
    "    medium = 0 \n",
    "\n",
    "    if (ts.numerator%3)==0:\n",
    "        medium = 3\n",
    "\n",
    "    elif (ts.numerator%2)==0:\n",
    "        medium = 2\n",
    "\n",
    "    ##  debugging\n",
    "    for idx in range(len(beat_sequence)):\n",
    "\n",
    "        # print('time idx', idx)\n",
    "\n",
    "        # Add 1 to each beat\n",
    "        if idx%((beatDuration/0.25))==0:\n",
    "            # print('adding 1 to beat sequence')\n",
    "            beat_sequence[idx] += 1\n",
    "\n",
    "        \n",
    "        # Mark medium-weight beat (at every second or third beat)\n",
    "        if (medium==3 and idx%((3*beatDuration/0.25))==0) or \\\n",
    "            (medium==2 and idx%((2*beatDuration/0.25))==0):\n",
    "            # print('adding 1 to beat sequence because medium')\t\n",
    "            beat_sequence[idx] += 1\n",
    "            \n",
    "    return beat_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definition of tokens\n",
    "- start_of_score_token =[0,0,0,0,0,0,0]\n",
    "- Intrument_tokens =    [1,0,0,0,0,0,SATB]\n",
    "- Start_sequence =      [2,0,0,0,0,0,0]\n",
    "- Note_on =             [3,beat=0-15,beat_str=0-3,position=number_pos_measure,pitch=midi_range,duration=0-16,SATB]\n",
    "- Chord_on =            [4,beat=0-15,beat_str=0-3,Chord_degree=1-7?,root=0-12,mode=[0=major,1=minor,2=dim,3=aug],extension=letssee]\n",
    "- end_score =           [5,0,0,0,0,0,0]\n",
    "\n",
    "Chord_modes:\n",
    "- 1 = major/dominant/\n",
    "- 2 = minor\n",
    "- 3 = diminished\n",
    "- 4 = augmented\n",
    "- 5 = suspended-fourth\n",
    "- 6 = suspended-second\n",
    "- 7 = power \n",
    "\n",
    "Chord Extensions:\n",
    "- 5 = regular\n",
    "- 6 = dimished-seventh\n",
    "- 7 = dominant\n",
    "- 8 = seventh\n",
    "- 8+2 =10 =major- ninth\n",
    "- 7+2 =9 minor-ninth\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "## different strings encountere from chord kind.split('-')[0]\n",
    "# major, minor, diminished, augmented, suspended,half,power,dominant\n",
    "\n",
    "def extract_chord_mode_extension(chord: m21.chord.Chord):\n",
    "    \n",
    "    chord_kind = chord.chordKind.split('-')\n",
    "    chord_kind_first = chord_kind[0]\n",
    "\n",
    "    if chord_kind_first=='major':\n",
    "\n",
    "        mode=1\n",
    "        if len(chord_kind)>1:\n",
    "            \n",
    "            ## managing different majors\n",
    "            if chord_kind[1]=='seventh':\n",
    "                extension=8\n",
    "            elif chord_kind[1]=='ninth':\n",
    "                extension=10\n",
    "        else:\n",
    "            extension=5\n",
    "            \n",
    "    elif chord_kind_first=='dominant':\n",
    "        mode=1\n",
    "\n",
    "        ## manage dominants\n",
    "        if len(chord_kind)>1:\n",
    "            if chord_kind[1]=='seventh':\n",
    "                extension=7\n",
    "            elif chord_kind[1]=='ninth':\n",
    "                extension=9\n",
    "        else:\n",
    "            extension=5\n",
    "        \n",
    "    elif chord_kind_first=='minor':\n",
    "        mode=2\n",
    "        if len(chord_kind)>1:\n",
    "            if chord_kind[1]=='seventh':\n",
    "                extension=7\n",
    "            elif chord_kind[1]=='major' and chord_kind[2]=='seventh':\n",
    "                extension=8\n",
    "        \n",
    "            elif chord_kind[1]=='ninth':\n",
    "                extension=9\n",
    "            elif chord_kind[1]=='major' and chord_kind[2]=='ninth':\n",
    "                extension=10\n",
    "        else:\n",
    "            extension=5\n",
    "            \n",
    "            \n",
    "    elif chord_kind_first=='diminished':\n",
    "        mode=3\n",
    "        if len(chord_kind)>1:\n",
    "            if chord_kind[1]=='seventh':\n",
    "                extension=6\n",
    "        else:\n",
    "            extension=5\n",
    "        \n",
    "    elif chord_kind_first=='augmented':\n",
    "        mode=4\n",
    "        if len(chord_kind)>1:\n",
    "            if chord_kind[1]=='major':\n",
    "                extension=8\n",
    "        else:\n",
    "            extension=5 \n",
    "    elif chord_kind_first=='suspended':\n",
    "        ## logic to manage sus 4 and sus 2\n",
    "        if chord_kind[1]=='fourth':\n",
    "            mode=5\n",
    "        else:\n",
    "            mode=6\n",
    "        extension=5\n",
    "    elif chord_kind_first=='half':\n",
    "        ## half diminished\n",
    "        mode=3\n",
    "        extension=7\n",
    "        pass\n",
    "    elif chord_kind_first=='power':\n",
    "        mode=7\n",
    "        extension=5\n",
    "    else:\n",
    "        mode=0\n",
    "        extension=0\n",
    "    return mode, extension\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody Reader\n",
    "Input\n",
    "- Music 21 parts, with melody and chords\n",
    "\n",
    "Output\n",
    "- tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def melody_reader(melody_part, gap,intrument,scale,chord_counter=None, chord_degree_counter=None):\n",
    "\n",
    "    # # Initialization\n",
    "    # melody_txt = []\n",
    "    # ts_seq = []\n",
    "    # beat_txt = []\n",
    "    # fermata_txt = []\n",
    "    # chord_txt = []\n",
    "    # chord_token = [0.]*12\n",
    "    # fermata_flag = False\n",
    "    melody_tokens = []\n",
    "\n",
    "    ### new Tokens\n",
    "    ## array of tokens as encountered\n",
    "    aot_encounter =[]\n",
    "\n",
    "\n",
    "    ## chord tokens\n",
    "    chord_tokens = []\n",
    "\n",
    " \n",
    "\n",
    "    ## beat definition strength\n",
    "    ### if beat is strong, i.e first beat 3\n",
    "    ## if beat is medium, i.e third beat 2\n",
    "    ## if beat is weak, i.e second beat and fourth 1\n",
    "    ## else is none\n",
    "\n",
    "    # Read note and meta information from melody part\n",
    "    for element in melody_part.flatten():\n",
    "       \n",
    "\n",
    "        ### Definition of tokens\n",
    "        token_encountered = [0] * 7\n",
    "        ## note: token_encounter[0]=3\n",
    "        ## chord:  token_encounter[0]=4\n",
    "\n",
    "\n",
    "        if isinstance(element, m21.note.Note):\n",
    "            # midi pitch as note onset\n",
    "            ## normalize to C\n",
    "            token_encountered[0]=3\n",
    "            midi_note = element.transpose(gap).pitch.midi\n",
    "\n",
    "            \n",
    "            beat_in_16ths = int(element.beat*4)\n",
    "            ## first beat\n",
    "            if beat_in_16ths == 4:\n",
    "                beat_strength = 3\n",
    "            elif beat_in_16ths == 8:\n",
    "                beat_strength = 1\n",
    "            elif beat_in_16ths == 12:\n",
    "                beat_strength = 2\n",
    "            elif beat_in_16ths == 16:\n",
    "                beat_strength = 1\n",
    "            else:\n",
    "                beat_strength = 0\n",
    "        \n",
    "            ## is offset position?\n",
    "            position = int(element.offset/0.25)\n",
    "            \n",
    "            duration = int(element.quarterLength*4)\n",
    "\n",
    "            token_encountered[1]=position\n",
    "            \n",
    "            token_encountered[2]=beat_in_16ths\n",
    "            token_encountered[3]=beat_strength\n",
    "            token_encountered[4]=midi_note\n",
    "            token_encountered[5]=duration\n",
    "            token_encountered[6]=intrument\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # for f in element.expressions:\n",
    "            #     if isinstance(f, m21.expressions.Fermata):\n",
    "            #         fermata_flag = True\n",
    "            #         break\n",
    "\n",
    "        elif isinstance(element, m21.note.Rest):\n",
    "            # 128 as rest onset\n",
    "            token_encountered[0]=3\n",
    "            token = 128\n",
    "            duration = int(element.quarterLength*4)\n",
    "            position = int(element.offset/0.25)\n",
    "            beat_in_16ths = int(element.beat*4)\n",
    "            if beat_in_16ths == 4:\n",
    "                beat_strength = 3\n",
    "            elif beat_in_16ths == 8:\n",
    "                beat_strength = 1\n",
    "            elif beat_in_16ths == 12:\n",
    "                beat_strength = 2\n",
    "            elif beat_in_16ths == 16:\n",
    "                beat_strength = 1\n",
    "            else:\n",
    "                beat_strength = 0\n",
    "            token_encountered[1]=position\n",
    "            \n",
    "            token_encountered[2]=beat_in_16ths\n",
    "            token_encountered[3]=beat_strength\n",
    "            token_encountered[4]=128\n",
    "            token_encountered[5]=duration\n",
    "            token_encountered[6]=intrument\n",
    "            \n",
    "        ## if chord, take the highest note\n",
    "        elif isinstance(element, m21.chord.Chord) and not isinstance(element, m21.harmony.ChordSymbol):\n",
    "            notes = [n.transpose(gap).pitch.midi for n in element.notes]\n",
    "            notes.sort()\n",
    "            midi_note = notes[-1]\n",
    "                        \n",
    "            beat_in_16ths = int(element.beat*4)\n",
    "            ## first beat\n",
    "            if beat_in_16ths == 4:\n",
    "                beat_strength = 3\n",
    "            elif beat_in_16ths == 8:\n",
    "                beat_strength = 1\n",
    "            elif beat_in_16ths == 12:\n",
    "                beat_strength = 2\n",
    "            elif beat_in_16ths == 16:\n",
    "                beat_strength = 1\n",
    "            else:\n",
    "                beat_strength = 0\n",
    "        \n",
    "            ## is offset position?\n",
    "            position = int(element.offset/0.25)\n",
    "            \n",
    "            duration = int(element.quarterLength*4)\n",
    "\n",
    "            token_encountered[1]=position\n",
    "            token_encountered[2]=beat_in_16ths\n",
    "            token_encountered[3]=beat_strength\n",
    "            token_encountered[4]=midi_note\n",
    "            token_encountered[5]=duration\n",
    "            token_encountered[6]=intrument\n",
    "\n",
    "\n",
    "            \n",
    "        elif isinstance(element, m21.harmony.ChordSymbol):\n",
    "            \n",
    "            \n",
    "            # m21.harmony.ChordSymbol.roo\n",
    "            ## STUDY HOW MANY DIFFERENT CHORD kinds\n",
    "            token_encountered[0]=4\n",
    "            element = element.transpose(gap)\n",
    "\n",
    "            degree = scale.getScaleDegreeAndAccidentalFromPitch(element.root())\n",
    "            root = element.root().midi %12\n",
    "            beat_in_16ths = int(element.beat*4)\n",
    "\n",
    "            ## paper extracts chords on strong beats, maybe not worth and better to have position\n",
    "            ## as a token\n",
    "\n",
    "            position= int(element.offset/0.25)\n",
    "            if beat_in_16ths == 4:\n",
    "                beat_strength = 3\n",
    "            elif beat_in_16ths == 8:\n",
    "                beat_strength = 1\n",
    "            elif beat_in_16ths == 12:\n",
    "                beat_strength = 2\n",
    "            elif beat_in_16ths == 16:\n",
    "                beat_strength = 1\n",
    "            else:\n",
    "                beat_strength = 0\n",
    "            \n",
    "            mode, extension = extract_chord_mode_extension(element)\n",
    "            if(mode==0):\n",
    "                print('what')\n",
    "                print(element.fullName)\n",
    "                continue\n",
    "            token_encountered[1]=beat_strength\n",
    "                                \n",
    "            token_encountered[2]=beat_in_16ths\n",
    "            token_encountered[3]=degree[0]\n",
    "            token_encountered[4]=root\n",
    "            token_encountered[5]=mode\n",
    "            token_encountered[6]=extension\n",
    "\n",
    "            aot_encounter.append(token_encountered)\n",
    "            chord_tokens.append(token_encountered)\n",
    "            # position = int(element.offset/0.25)\n",
    "    \t    ### to extract relevant data for tokenization. \n",
    "            if(chord_counter is not None):\n",
    "                if element.chordKind not in chord_counter:\n",
    "                    chord_counter[element.chordKind] = 0\n",
    "                chord_counter[element.chordKind] += 1\n",
    "            if(chord_degree_counter is not None):\n",
    "                ## getting chord degree\n",
    "\n",
    "                degree = scale.getScaleDegreeFromPitch(element.root(), comparisonAttribute='pitchClass')\n",
    "                degree2 = scale.getScaleDegreeAndAccidentalFromPitch(element.root())\n",
    "                if degree is not None:\n",
    "                    if degree not in chord_degree_counter['scale_degree_without']:\n",
    "                        chord_degree_counter['scale_degree_without'][degree] = 0\n",
    "                    chord_degree_counter['scale_degree_without'][degree] += 1\n",
    "                else:\n",
    "                    if 'None' not in chord_degree_counter['scale_degree_without']:\n",
    "                        chord_degree_counter['scale_degree_without']['None'] = {}\n",
    "                    if element.root() not in chord_degree_counter['scale_degree_without']['None']:\n",
    "                        chord_degree_counter['scale_degree_without']['None'][element.root()] = 0\n",
    "                    chord_degree_counter['scale_degree_without']['None'][element.root()] +=1\n",
    "                if degree2 is not None:\n",
    "                    if degree2[0] not in chord_degree_counter['scale_degree_acc']:\n",
    "                        chord_degree_counter['scale_degree_acc'][degree2[0]] = {}\n",
    "                    \n",
    "                    \n",
    "                    if degree2[1] is not None:\n",
    "                        if degree2[1].name not in chord_degree_counter['scale_degree_acc'][degree2[0]]:\n",
    "\n",
    "                            chord_degree_counter['scale_degree_acc'][degree2[0]][degree2[1].name]=0\n",
    "                        chord_degree_counter['scale_degree_acc'][degree2[0]][degree2[1].name] += 1\n",
    "                    else:\n",
    "                        if 'None' not in chord_degree_counter['scale_degree_acc'][degree2[0]]:\n",
    "                            chord_degree_counter['scale_degree_acc'][degree2[0]]['None'] = 0\n",
    "                        chord_degree_counter['scale_degree_acc'][degree2[0]]['None'] += 1\n",
    "                else:\n",
    "                    print('degree2 is none')\n",
    "            \n",
    "\n",
    " \n",
    "            continue\n",
    "\n",
    "        # Read the current time signature\n",
    "        elif isinstance(element, m21.meter.TimeSignature):\n",
    "\n",
    "            # ts_seq.append(element)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if element.quarterLength==0:\n",
    "            continue\n",
    "        \n",
    "        aot_encounter.append(token_encountered)\n",
    "        \n",
    "        melody_tokens.append(token_encountered)\n",
    "\n",
    "\n",
    "    return aot_encounter,melody_tokens,chord_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING THE CODE ABOVE/ create token sequences\n",
    "\n",
    "Things to check for, is chord position important? should i encode chord position instead of beat strength?\n",
    "or is positional encoding enough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 204\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_convert.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(file):\n\u001b[1;32m    200\u001b[0m         \n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m##amount of files\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m         files \u001b[38;5;241m=\u001b[39m \u001b[43mget_filenames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmount of files extracted: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\u001b[38;5;28mlen\u001b[39m(files))\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# Split 80,10,10\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m, in \u001b[0;36mget_filenames\u001b[0;34m(input_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, this_file)\n\u001b[0;32m---> 17\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mm21\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m skippable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#   ## Added, if the score has a 3/4 time signature, we will skip it.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/converter/__init__.py:1297\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(value, forceSource, number, format, **keywords)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parseData(value, number\u001b[38;5;241m=\u001b[39mnumber, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mbytes\u001b[39m)\n\u001b[1;32m   1296\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m _osCanLoad(valueStr)):\n\u001b[0;32m-> 1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalueStr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mforceSource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforceSource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mbytes\u001b[39m)\n\u001b[1;32m   1300\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m _osCanLoad(common\u001b[38;5;241m.\u001b[39mcleanpath(valueStr))):\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parseFile(common\u001b[38;5;241m.\u001b[39mcleanpath(valueStr), number\u001b[38;5;241m=\u001b[39mnumber, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m   1302\u001b[0m                      forceSource\u001b[38;5;241m=\u001b[39mforceSource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/converter/__init__.py:1151\u001b[0m, in \u001b[0;36mparseFile\u001b[0;34m(fp, number, format, forceSource, **keywords)\u001b[0m\n\u001b[1;32m   1149\u001b[0m v \u001b[38;5;241m=\u001b[39m Converter()\n\u001b[1;32m   1150\u001b[0m fp \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mcleanpath(fp, returnPathlib\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1151\u001b[0m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforceSource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforceSource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mTYPE_CHECKING:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v\u001b[38;5;241m.\u001b[39mstream, (stream\u001b[38;5;241m.\u001b[39mScore, stream\u001b[38;5;241m.\u001b[39mPart, stream\u001b[38;5;241m.\u001b[39mOpus))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/converter/__init__.py:608\u001b[0m, in \u001b[0;36mConverter.parseFile\u001b[0;34m(self, fp, number, format, forceSource, storePickle, **keywords)\u001b[0m\n\u001b[1;32m    606\u001b[0m environLocal\u001b[38;5;241m.\u001b[39mprintDebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading Pickled version\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thawedStream \u001b[38;5;241m=\u001b[39m \u001b[43mthaw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpPickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzipType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzlib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m freezeThaw\u001b[38;5;241m.\u001b[39mFreezeThawException:\n\u001b[1;32m    610\u001b[0m     environLocal\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not parse pickle, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpPickle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...rewriting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/converter/__init__.py:1378\u001b[0m, in \u001b[0;36mthaw\u001b[0;34m(fp, zipType)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmusic21\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m freezeThaw\n\u001b[1;32m   1377\u001b[0m v \u001b[38;5;241m=\u001b[39m freezeThaw\u001b[38;5;241m.\u001b[39mStreamThawer()\n\u001b[0;32m-> 1378\u001b[0m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzipType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzipType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstream\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/freezeThaw.py:938\u001b[0m, in \u001b[0;36mStreamThawer.open\u001b[0;34m(self, fp, zipType)\u001b[0m\n\u001b[1;32m    936\u001b[0m             common\u001b[38;5;241m.\u001b[39mrestorePathClassesAfterUnpickling()\n\u001b[1;32m    937\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m FreezeThawException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown zipType \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzipType\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 938\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpackStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m     common\u001b[38;5;241m.\u001b[39mrestorePathClassesAfterUnpickling()\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjsonpickle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/freezeThaw.py:887\u001b[0m, in \u001b[0;36mStreamThawer.unpackStream\u001b[0;34m(self, storage)\u001b[0m\n\u001b[1;32m    884\u001b[0m     environLocal\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis pickled file is out of date and may not function properly.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    885\u001b[0m streamObj \u001b[38;5;241m=\u001b[39m storage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardownSerializationScaffold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreamObj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m streamObj\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/freezeThaw.py:771\u001b[0m, in \u001b[0;36mStreamThawer.teardownSerializationScaffold\u001b[0;34m(self, streamObj)\u001b[0m\n\u001b[1;32m    768\u001b[0m storedAutoSort \u001b[38;5;241m=\u001b[39m streamObj\u001b[38;5;241m.\u001b[39mautoSort\n\u001b[1;32m    769\u001b[0m streamObj\u001b[38;5;241m.\u001b[39mautoSort \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestoreElementsFromTuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreamObj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestoreStreamStatusClient(streamObj)\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# removing seems to create problems for jsonPickle with Spanners\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/freezeThaw.py:867\u001b[0m, in \u001b[0;36mStreamThawer.restoreElementsFromTuples\u001b[0;34m(self, streamObj)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subElement \u001b[38;5;129;01min\u001b[39;00m streamObj:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subElement\u001b[38;5;241m.\u001b[39misStream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# note that the elements may have already been restored\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# if the spanner stores a part or something in the Stream\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;66;03m# for instance in a StaffGroup object\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestoreElementsFromTuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubElement\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/freezeThaw.py:867\u001b[0m, in \u001b[0;36mStreamThawer.restoreElementsFromTuples\u001b[0;34m(self, streamObj)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subElement \u001b[38;5;129;01min\u001b[39;00m streamObj:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subElement\u001b[38;5;241m.\u001b[39misStream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# note that the elements may have already been restored\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# if the spanner stores a part or something in the Stream\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;66;03m# for instance in a StaffGroup object\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestoreElementsFromTuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubElement\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/freezeThaw.py:860\u001b[0m, in \u001b[0;36mStreamThawer.restoreElementsFromTuples\u001b[0;34m(self, streamObj)\u001b[0m\n\u001b[1;32m    858\u001b[0m             streamObj\u001b[38;5;241m.\u001b[39mcoreStoreAtEnd(e)\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m streamObj\u001b[38;5;241m.\u001b[39m_storedElementOffsetTuples\n\u001b[0;32m--> 860\u001b[0m     \u001b[43mstreamObj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoreElementsChanged\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subElement \u001b[38;5;129;01min\u001b[39;00m streamObj:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subElement\u001b[38;5;241m.\u001b[39misStream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# note that the elements may have already been restored\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# if the spanner stores a part or something in the Stream\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;66;03m# for instance in a StaffGroup object\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/stream/core.py:268\u001b[0m, in \u001b[0;36mStreamCore.coreElementsChanged\u001b[0;34m(self, updateIsFlat, clearIsSorted, memo, keepIndex)\u001b[0m\n\u001b[1;32m    263\u001b[0m         origin\u001b[38;5;241m.\u001b[39mclearCache()\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# may not always need to clear cache of all living sites, but may\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# always be a good idea since .flatten() has changed etc.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# should not need to do derivation.origin sites.\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m livingSite \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msites:\n\u001b[1;32m    269\u001b[0m     livingSite\u001b[38;5;241m.\u001b[39mcoreElementsChanged(memo\u001b[38;5;241m=\u001b[39mmemo)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# clear these attributes for setting later\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/music21/sites.py:476\u001b[0m, in \u001b[0;36mSites.yieldSites\u001b[0;34m(self, excludeNone, sortByCreationTime, priorityTarget)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# get each dict from all defined contexts\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keyRepository:\n\u001b[0;32m--> 476\u001b[0m     siteRef \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msiteDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# check for None object; default location, not a weakref, keep\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m siteRef\u001b[38;5;241m.\u001b[39msite \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def convert_files(filenames, fromDataset=True):\n",
    "\n",
    "    print('\\nConverting %d files...' %(len(filenames)))\n",
    "\n",
    "    ## EACH SEQUENCE IS A SONG\n",
    "\n",
    "    complete_melody_with_chords_sequences= []\n",
    "    complete_soprano_melody_sequences=[]\n",
    "    complete_chords_sequences = []\n",
    "\n",
    "    ## what is data corpus\n",
    "    data_corpus = []\n",
    "\n",
    "    complete_alto_sequences_mel= []\n",
    "    complete_tenor_sequences_mel = []\n",
    "    complete_bass_sequences_mel = []\n",
    "\n",
    "    complete_three_sequences_voice_mix = []\n",
    "\n",
    "    # study_chord_kind = {}\n",
    "    scale = m21.scale.MajorScale('C')\n",
    "\n",
    "    # study_chord_degrees = {}\n",
    "\n",
    "    # if isinstance(study_chord_degrees, dict):\n",
    "    #     study_chord_degrees['scale_degree_acc']={}\n",
    "    #     study_chord_degrees['scale_degree_without']={}\n",
    "\n",
    "\n",
    "    for filename_idx in trange(len(filenames)):\n",
    "\n",
    "        # Read this music file\n",
    "        filename = filenames[filename_idx]\n",
    "\n",
    "        # Ensure that suffixes are valid\n",
    "        if os.path.splitext(filename)[-1] not in EXTENSION:\n",
    "            continue\n",
    "\n",
    "        # try:\n",
    "        # Read this music file\n",
    "        score = m21.converter.parse(filename)\n",
    "\n",
    "        # Read each part\n",
    "        ## LOop over  score parts, our case 4 parts. \n",
    "        # print('song # %d' %filename_idx)\n",
    "\n",
    "\n",
    "        ## for each song we are creating an array that contains\n",
    "        ## token arrays. \n",
    "\n",
    "        this_song_soprano_w_chords = []\n",
    "        this_song_soprano_melody =[]\n",
    "        this_song_alto_melody = []\n",
    "        this_song_tenor_melody = []\n",
    "        this_song_bass_melody = []\n",
    "\n",
    "        this_three_voice_mix = []\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "        for idx, part in enumerate(score.parts):\n",
    "            \n",
    "\n",
    "                \n",
    "            part = quant_score(part)\n",
    "            # print('before splitting. part for idx %d' %idx)\n",
    "            ## There is no split exxtra, everything seems normal.\n",
    "            splited_score, gap_list = split_by_key(part)\n",
    "            # print(\"splited_score\")\n",
    "            # print(splited_score)\n",
    "            # print('gap_list')\n",
    "            # print(gap_list)\n",
    "\n",
    "            # print(len(splited_score))\n",
    "            # continue\n",
    "            if idx==0:\n",
    "\n",
    "                \n",
    "\n",
    "                # Convert soprano \n",
    "                ## Nota al calze, even if in loop, len is always 1. \n",
    "                for s_idx in range(len(splited_score)):\n",
    "                    melody_part = splited_score[s_idx]\n",
    "\n",
    "                    ## changed all codes\n",
    "                    this_song_soprano_w_chords, this_song_soprano_melody, this_song_chords = melody_reader(melody_part, gap_list[s_idx], 0,scale,chord_counter=None,chord_degree_counter=None)\n",
    "                    \n",
    "                    ##adding important tokens to \n",
    "                    this_song_soprano_w_chords.insert(0,[0]*7)\n",
    "                    this_song_soprano_w_chords.insert(1,[1,0,0,0,0,0,1])\n",
    "                    this_song_soprano_w_chords.insert(2, [2,0,0,0,0,0,0])\n",
    "                    this_song_soprano_w_chords.append([5,0,0,0,0,0,0])\n",
    "\n",
    "            \n",
    "                    ## adding to mel\n",
    "                    this_song_soprano_melody.insert(0,[0]*7)\n",
    "                    this_song_soprano_melody.insert(1,[1,0,0,0,0,0,1])\n",
    "                    this_song_soprano_melody.insert(2, [2,0,0,0,0,0,0])\n",
    "                    this_song_soprano_melody.append([5,0,0,0,0,0,0])\n",
    "\n",
    "                    ## adding to chord sequence\n",
    "                    this_song_chords.insert(0,[0]*7)\n",
    "                    this_song_chords.insert(1,[2,0,0,0,0,0,0])\n",
    "                    this_song_chords.append([5,0,0,0,0,0,0])\n",
    "\n",
    "               \n",
    "                    \n",
    "                    complete_melody_with_chords_sequences.append(this_song_soprano_w_chords.copy())\n",
    "                    complete_soprano_melody_sequences.append(this_song_soprano_melody.copy())\n",
    "                    complete_chords_sequences.append(this_song_chords.copy())\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                    \n",
    "                    # Convert alto, tenor and bass\n",
    "                for s_idx in range(len(splited_score)):\n",
    "                    melody_part = splited_score[s_idx]\n",
    "                    ## CHANGE code, interestignly enough, will different instruments be trained at the same time?\n",
    "                    complete,melody_part,_ = melody_reader(melody_part, gap_list[s_idx],idx,scale,None,None)\n",
    "\n",
    "\n",
    "                if idx==1:\n",
    "                    temp_alto_mel =melody_part.copy()\n",
    "                    temp_alto_mel.insert(0,[0]*7)\n",
    "                    temp_alto_mel.insert(1,[1,0,0,0,0,0,idx])\n",
    "                    temp_alto_mel.insert(2, [2,0,0,0,0,0,0])\n",
    "                    temp_alto_mel.append([5,0,0,0,0,0,0])\n",
    "                    complete_alto_sequences_mel.append(temp_alto_mel.copy())\n",
    "\n",
    "\n",
    "                elif idx==2:\n",
    "                    temp_tenor_mel =melody_part.copy()\n",
    "                    temp_tenor_mel.insert(0,[0]*7)\n",
    "                    temp_tenor_mel.insert(1,[1,0,0,0,0,0,idx])\n",
    "                    temp_tenor_mel.insert(2, [2,0,0,0,0,0,0])\n",
    "                    temp_tenor_mel.append([5,0,0,0,0,0,0])\n",
    "                    complete_tenor_sequences_mel.append(temp_tenor_mel.copy())\n",
    "\n",
    "\n",
    "                elif idx==3:\n",
    "                    temp_bass_mel = melody_part.copy()\n",
    "                    temp_bass_mel.insert(0,[0]*7)\n",
    "                    temp_bass_mel.insert(1,[1,0,0,0,0,0,idx])\n",
    "                    temp_bass_mel.insert(2, [2,0,0,0,0,0,0])\n",
    "                    temp_bass_mel.append([5,0,0,0,0,0,0])\n",
    "                    complete_bass_sequences_mel.append(temp_bass_mel.copy())\n",
    "\n",
    "\n",
    "                this_three_voice_mix+=melody_part\n",
    "\n",
    "        \n",
    "        # ## After all_parts have been read, add the end and start three voice\n",
    "            \n",
    "        # temp_three_voice_mix.sort(key=lambda x: x[1])\n",
    "\n",
    "\n",
    "        this_three_voice_mix.insert(0,[0]*7)\n",
    "        this_three_voice_mix.insert(1,[1,0,0,0,0,0,2])\n",
    "        this_three_voice_mix.insert(2,[1,0,0,0,0,0,3])\n",
    "        this_three_voice_mix.insert(3,[1,0,0,0,0,0,4])\n",
    "        this_three_voice_mix.insert(4, [2,0,0,0,0,0,0])\n",
    "        this_three_voice_mix.append([5,0,0,0,0,0,0])\n",
    "\n",
    "        complete_three_sequences_voice_mix.append(this_three_voice_mix.copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_sequence_dict ={\n",
    "        'all_parts': complete_melody_with_chords_sequences,\n",
    "        'soprano': complete_soprano_melody_sequences,\n",
    "        'chords': complete_chords_sequences\n",
    "    } \n",
    "    ##output sequence is\n",
    "\n",
    "    output_sequence_dict ={\n",
    "        'alto': complete_alto_sequences_mel,\n",
    "        'tenor': complete_tenor_sequences_mel,\n",
    "        'bass': complete_bass_sequences_mel,\n",
    "        'all_parts': complete_three_sequences_voice_mix\n",
    "    }\n",
    "    \n",
    " \n",
    "    return input_sequence_dict, output_sequence_dict\n",
    "\n",
    "\n",
    "    # data_corpus.append((input_sequence, output_sequence))\n",
    "with open('output_convert.txt', 'w') as file:\n",
    "    with redirect_stdout(file):\n",
    "        \n",
    "\n",
    "        ##amount of files\n",
    "\n",
    "        files = get_filenames(DATASET_PATH)\n",
    "\n",
    "        print('Amount of files extracted: %d' %len(files))\n",
    "\n",
    "        # Split 80,10,10\n",
    "        train_files = files[:int(len(files)*0.8)]\n",
    "        val_files = files[int(len(files)*0.8):int(len(files)*0.9)]\n",
    "        test_files = files[int(len(files)*0.9):]\n",
    "\n",
    "        \n",
    "        input_seq,output_seq = convert_files(train_files, fromDataset=False)\n",
    "        with open('train_input_sequence.pkl', 'wb') as f:\n",
    "            pickle.dump(input_seq, f)\n",
    "        with open('train_output_sequence.pkl', 'wb') as f:\n",
    "            pickle.dump(output_seq, f)\n",
    "\n",
    "        input_seq,output_seq = convert_files(test_files, fromDataset=False)\n",
    "        with open('test_input_sequence.pkl', 'wb') as f:\n",
    "            pickle.dump(input_seq, f)\n",
    "        with open('test_output_sequence.pkl', 'wb') as f:\n",
    "            pickle.dump(output_seq, f)\n",
    "    \n",
    "        input_seq,output_seq = convert_files(val_files, fromDataset=False)\n",
    "        with open('val_input_sequence.pkl', 'wb') as f:\n",
    "            pickle.dump(input_seq, f)\n",
    "        with open('val_output_sequence.pkl', 'wb') as f:\n",
    "            pickle.dump(output_seq, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences, ready for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences for transformer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def return_padded_sequences(song):\n",
    "    \n",
    "    # Determine the maximum sequence length\n",
    "    max_length = max(len(seq) for seq in song)\n",
    "\n",
    "    # Pad sequences to the maximum length\n",
    "    padded_sequences = [\n",
    "        seq + [[6, 0,0,0,0,0,0]] * (max_length - len(seq))  # Pad with [0, 0]\n",
    "        for seq in song\n",
    "    ]\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "type_of_data = ['train_', 'val_', 'test_']\n",
    "all_songs_desafinau ={\n",
    "    'input':{},\n",
    "    'output':{}\n",
    "}\n",
    "for t in type_of_data:\n",
    "    input_seq = pickle.load(open(t+'input_sequence.pkl', 'rb'))\n",
    "    output_seq = pickle.load(open(t+'output_sequence.pkl', 'rb'))\n",
    "\n",
    "    all_songs_desafinau['input'][t]={}\n",
    "    all_songs_desafinau['output'][t]={}\n",
    "    for key, i in input_seq.items():\n",
    "        sequences_of_songs =return_padded_sequences(i)\n",
    "        sos = np.array(sequences_of_songs)\n",
    "        all_songs_desafinau['input'][t][key]=sos\n",
    "        # print(song_counter)\n",
    "    for key, i in output_seq.items():\n",
    "        sequences_of_songs =return_padded_sequences(i)\n",
    "        sos = np.array(sequences_of_songs)\n",
    "        # print(sos.shape)\n",
    "        all_songs_desafinau['output'][t][key]=sos\n",
    "        # print(song_counter)\n",
    "\n",
    "\n",
    "with open('all_songs_split.pkl', 'wb') as f:\n",
    "    pickle.dump(all_songs_desafinau, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Relevant Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_kind_chords ={'major': 8898,\n",
    " 'minor': 4559,\n",
    " 'minor-seventh': 604, \n",
    " 'diminished': 501, \n",
    " 'dominant-seventh': 902, \n",
    " 'diminished-seventh': 190, \n",
    " 'augmented': 45, \n",
    " 'suspended-fourth': 680, \n",
    " 'half-diminished-seventh': 412, \n",
    " 'minor-ninth': 14, \n",
    " 'augmented-major-seventh': 18,\n",
    " 'power': 48, \n",
    " 'major-seventh': 126, \n",
    " 'suspended-second': 115, \n",
    " 'major-ninth': 16, \n",
    " 'minor-major-seventh': 2, \n",
    " 'dominant-ninth': 4, \n",
    " 'minor-major-ninth': 1}\n",
    "\n",
    "## Legend\n",
    "\n",
    "\n",
    "scale_degree_acc= {\n",
    "    1: {\n",
    "        'None': 3866, \n",
    "        'sharp': 126}, \n",
    "    6: {'None': 3227}, \n",
    "    4: {'None': 1507, 'sharp': 132}, \n",
    "    2: {'None': 2505}, \n",
    "    5: {'None': 2908, 'sharp': 180}, \n",
    "    7: {'None': 695, 'flat': 173}, \n",
    "    3: {'None': 1753, 'flat': 63}\n",
    "    }\n",
    "\n",
    "\n",
    "### Total\n",
    "input_seq_pk={\n",
    "'all_parts':38275,\n",
    "'soprano':21140,\n",
    "'chords':18110\n",
    "}\n",
    "\n",
    "output_seq_pk={\n",
    "    'alto':24303,\n",
    "'tenor':25068,\n",
    "'bass':25331,\n",
    "'all_parts':72752,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader for complete sequence\n",
    "- Input: [soprano,chord]\n",
    "- output: [alto,tenor,bass]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "## Input is mixed (soprano and chords)\n",
    "## output is mixed alto, tenor and bass\n",
    "\n",
    "class MusicDatasetComplete(Dataset):\n",
    "    def __init__(self, input_sequences, output_sequences):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.output_sequences = output_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.input_sequences[idx]\n",
    "        output_seq = self.output_sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.float), torch.tensor(output_seq, dtype=torch.float)\n",
    "\n",
    "## Testing dataset\n",
    "# input_seq = pickle.load(open('train_input_sequence.pkl', 'rb'))\n",
    "# output_seq = pickle.load(open('train_output_sequence.pkl', 'rb'))\n",
    "# dataset = MusicDatasetComplete(input_seq['all_parts'], output_seq['all_parts'])\n",
    "\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "# count=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to transform data to sequences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation, token error rate, there has to be another way to objective evaluate....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_token_error_rate(predicted, target):\n",
    "#     \"\"\"\n",
    "#     Calculate the token error rate between predicted and target sequences.\n",
    "    \n",
    "#     Args:\n",
    "#     predicted (torch.Tensor): The predicted token sequences.\n",
    "#     target (torch.Tensor): The target token sequences.\n",
    "    \n",
    "#     Returns:\n",
    "#     float: The token error rate.\n",
    "#     \"\"\"\n",
    "#     # Ensure the predicted and target sequences have the same shape\n",
    "#     assert predicted.shape == target.shape, \"Shape mismatch between predicted and target sequences\"\n",
    "    \n",
    "#     # Calculate the number of errors\n",
    "#     errors = (predicted != target).sum().item()\n",
    "    \n",
    "#     # Calculate the total number of tokens\n",
    "#     total_tokens = target.numel()\n",
    "    \n",
    "#     # Calculate the token error rate\n",
    "#     token_error_rate = errors / total_tokens\n",
    "    \n",
    "#     return token_error_rate\n",
    "\n",
    "# # Example usage\n",
    "# predicted = torch.tensor([\n",
    "#     [[18, 19, 20], [21, 22, 23], [24, 25, 26]],\n",
    "#     [[27, 28, 29], [30, 31, 32], [33, 34, 35]]\n",
    "# ])\n",
    "\n",
    "# target = torch.tensor([\n",
    "#     [[18, 19, 20], [21, 22, 23], [24, 25, 26]],\n",
    "#     [[27, 28, 29], [30, 31, 32], [33, 34, 36]],\n",
    "#     [[27, 2outputs8, 29], [30, 31, 32], [33, 34, 36]]  # Note the last token is different\n",
    "# ])\n",
    "\n",
    "# token_error_rate = calculate_token_error_rate(predicted, target)\n",
    "# print(f\"Token Error Rate: {token_error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUEEENOOO, after model definition, model creation and training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'output'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from gaby_deeplearning.model_architecture import *\n",
    "# import wandb\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "## embdedding size Según paper hay 128, 356\n",
    "hidden_size = [128, 256]\n",
    "\n",
    "##number of transformer blocks., hyper parameter to choose.\n",
    "num_layers = [(1, 1), (2, 3)]\n",
    "\n",
    "## number of heads, Según paper2 4\n",
    "num_heads = 4\n",
    "\n",
    "## segun paper, iteration=110\n",
    "num_iterations = 110\n",
    "\n",
    "# learning rate segun paper2\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# num_embeddings, interesting question (paper recommends 2048)\n",
    "num_embeddings = [1024, 2048, 4096]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##loading pickle for input and outputimport wandb\n",
    "\n",
    "\n",
    "positional_encodings = [\"sinusoidal\", \"libro\", None]\n",
    "\n",
    "\n",
    "##loading pickle for input and output\n",
    "all_inputs = pickle.load(open('all_songs_split.pkl', 'rb'))\n",
    "all_inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an embeddor for multiple token types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoderv1(\n",
      "  (encoder): Encoder(\n",
      "    (musical_embedding): MusicalEmbeddings(\n",
      "      (feature_extractor_embeddings): Embedding(7, 128)\n",
      "      (linear_reg_features): Linear(in_features=6, out_features=128, bias=True)\n",
      "      (concat_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (pos_embedding): SinusoidalPosEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (attn1): AttentionBlock(\n",
      "          (multihead_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm_mlp): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_mlp): Dropout(p=0.2, inplace=False)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (musical_embedding): MusicalEmbeddings(\n",
      "      (feature_extractor_embeddings): Embedding(7, 128)\n",
      "      (linear_reg_features): Linear(in_features=6, out_features=128, bias=True)\n",
      "      (concat_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (pos_embedding): SinusoidalPosEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (attn1): AttentionBlock(\n",
      "          (multihead_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (cross_attention): AttentionBlock(\n",
      "          (multihead_attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm_mlp): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_mlp): Dropout(p=0.2, inplace=False)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([48, 510, 7])\n",
      "torch.Size([48, 1107, 7])\n",
      "input_key_mask\n",
      "torch.Size([48, 510])\n",
      "torch.Size([48, 510, 7])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6], dtype=torch.int32)\n",
      "torch.Size([48, 510, 6])\n",
      "torch.Size([48, 510, 128])\n",
      "key_padding_mask\n",
      "torch.Size([48, 510])\n",
      "torch.Size([48, 1107, 7])\n",
      "tensor([0, 1, 2, 3, 5, 6], dtype=torch.int32)\n",
      "torch.Size([48, 1107, 6])\n",
      "key_padding_mask\n",
      "torch.Size([48, 1107])\n",
      "mask\n",
      "torch.Size([1107, 1107])\n",
      "key_padding_mask\n",
      "torch.Size([48, 510])\n",
      "shapes\n",
      "torch.Size([48, 1107, 7])\n",
      "torch.Size([48, 1107, 7])\n",
      "decoded_seq shape after reshaping: torch.Size([53136, 7])\n",
      "output_seq shape after reshaping: torch.Size([53136, 7])\n",
      "torch.Size([48, 510, 7])\n",
      "torch.Size([48, 1107, 7])\n",
      "input_key_mask\n",
      "torch.Size([48, 510])\n",
      "torch.Size([48, 510, 7])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6], dtype=torch.int32)\n",
      "torch.Size([48, 510, 6])\n",
      "torch.Size([48, 510, 128])\n",
      "key_padding_mask\n",
      "torch.Size([48, 510])\n",
      "torch.Size([48, 1107, 7])\n",
      "tensor([0, 1, 2, 3, 5, 6], dtype=torch.int32)\n",
      "torch.Size([48, 1107, 6])\n",
      "key_padding_mask\n",
      "torch.Size([48, 1107])\n",
      "mask\n",
      "torch.Size([1107, 1107])\n",
      "key_padding_mask\n",
      "torch.Size([48, 510])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = MusicDatasetComplete(all_inputs['input']['train_']['all_parts'], all_inputs['output']['train_']['all_parts'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_INPUT_TOKENS = 7\n",
    "NUM_OUTPUT_TOKENS = 6\n",
    "FEATURES_INSIDE_ARRAY=6\n",
    "\n",
    "test_dataset = MusicDatasetComplete(all_inputs['input']['test_']['all_parts'], all_inputs['output']['test_']['all_parts'])\n",
    "val_dataset = MusicDatasetComplete(all_inputs['input']['test_']['all_parts'], all_inputs['output']['test_']['all_parts'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=48, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=48, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=48, shuffle=False)\n",
    "\n",
    "# print(next(iter(train_dataloader)))\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "for h_s in hidden_size:\n",
    "    for n_l in num_layers:\n",
    "\n",
    "        for p_enc in positional_encodings:\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            model = EncoderDecoderv1(\n",
    "                number_of_tokens_input=NUM_INPUT_TOKENS,\n",
    "                number_of_tokens_output=NUM_INPUT_TOKENS,\n",
    "                feature_size=FEATURES_INSIDE_ARRAY,\n",
    "                \n",
    "                hidden_size=h_s,\n",
    "                num_heads=num_heads,\n",
    "                num_layers=n_l,\n",
    "                positional_encoding=p_enc,\n",
    "            )\n",
    "            model.to(device)\n",
    "            print(model)\n",
    "\n",
    "            ## wandb\n",
    "            # wandb.init(\n",
    "            #     # set the wandb project where this run will be logged\n",
    "            #     project=\"model_music_train\",\n",
    "            #     config={print\n",
    "            #         \"learning_rate\": 0.001,\n",
    "            #         \"architecture\": \"CNN\",\n",
    "            #         \"hidden_size\": h_s,\n",
    "            #         \"num_layers\": n_l,\n",
    "            #         \"num_heads\": num_heads,\n",
    "            #         \"num_embeddings\": n_emb,\n",
    "            #         \"position_encoding\": p_enc,\n",
    "            #     },\n",
    "            # )\n",
    "\n",
    "            ## Choosing cross entropy because chord thing does so\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            ## optimizer\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for batch_indx, (input_seq, output_seq) in enumerate(train_dataloader):\n",
    "                    input_seq, output_seq = input_seq.to(device), output_seq.to(device)\n",
    "                    print(input_seq.shape)\n",
    "                    print(output_seq.shape)\n",
    "                    # break\n",
    "\n",
    "\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    decoded_seq = model(input_seq,output_seq)\n",
    "\n",
    "                    print('shapes')\n",
    "                    print(decoded_seq.shape)\n",
    "                    print(output_seq.shape)\n",
    "\n",
    "                                        # Reshape decoded_seq and output_seq\n",
    "                    decoded_seq = decoded_seq.view(-1, decoded_seq.size(-1))  # Shape: [batch_size * sequence_length, num_classes]\n",
    "                    output_seq = output_seq.view(-1, output_seq.size(-1))  # Shape: [batch_size * sequence_length, num_classes]\n",
    "                    # Print shapes after reshaping\n",
    "                    print(\"decoded_seq shape after reshaping:\", decoded_seq.shape)\n",
    "                    print(\"output_seq shape after reshaping:\", output_seq.shape)\n",
    "\n",
    "                    # Compute the loss\n",
    "\n",
    "                    loss = criterion(\n",
    "                       decoded_seq, output_seq\n",
    "                    )\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                # Log the average loss for this epoch\n",
    "                avg_loss = running_loss / len(dataloader)\n",
    "                # wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement \"we apply different embeddings for the different features sharing the same axis\" means that for each type of token (e.g., note_token and chord_token), you use separate embedding layers to transform the tokens into their respective embeddings. This is useful when different types of tokens have different meanings and should be embedded differently.\n",
    "\n",
    "In your example, each token is characterized by the first index, and the other indices contain different information. You can use separate embedding layers for note_token and chord_token to ensure that each type of token is embedded appropriately.\n",
    "\n",
    "Example Implementation\n",
    "Let's assume you have two types of tokens: note_token and chord_token. You can create separate embedding layers for each type and apply them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examplecode might use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, num_note_embeddings, num_chord_embeddings, hidden_size):\n",
    "        super(MultiFeatureEmbedding, self).__init__()\n",
    "        self.note_embedding = nn.Embedding(num_note_embeddings, hidden_size)\n",
    "        self.chord_embedding = nn.Embedding(num_chord_embeddings, hidden_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: (batch_size, seq_len, feature_dim)\n",
    "        batch_size, seq_len, feature_dim = tokens.shape\n",
    "\n",
    "        # Separate the tokens based on the first index\n",
    "        note_tokens = tokens[:, :, 0]  # Assuming the first index indicates the type\n",
    "        chord_tokens = tokens[:, :, 1]  # Assuming the second index indicates the type\n",
    "\n",
    "        # Apply the respective embeddings\n",
    "        note_emb = self.note_embedding(note_tokens)\n",
    "        chord_emb = self.chord_embedding(chord_tokens)\n",
    "\n",
    "        # Combine the embeddings (e.g., concatenate or add)\n",
    "        combined_emb = note_emb + chord_emb  # Example: element-wise addition\n",
    "\n",
    "        return combined_emb\n",
    "\n",
    "# Example usage\n",
    "num_note_embeddings = 10\n",
    "num_chord_embeddings = 10\n",
    "hidden_size = 128\n",
    "\n",
    "model = MultiFeatureEmbedding(num_note_embeddings, num_chord_embeddings, hidden_size)\n",
    "\n",
    "# Example input tensor (batch_size, seq_len, feature_dim)\n",
    "input_tokens = torch.tensor([\n",
    "    [[1, 2, 3, 4], [2, 2, 3, 4]],  # Batch 1\n",
    "    [[1, 2, 3, 4], [2, 2, 3, 4]]   # Batch 2\n",
    "])\n",
    "\n",
    "output = model(input_tokens)\n",
    "print(output.shape)  # Output shape: (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.4' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/gabri/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Interesting statement occured, ddifferent tokens are  embeddeded in a different space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://youtu.be/Kujr55fNbGo?si=tw16yQxev55ezIDx'>Fundamental</a>\n",
    "\n",
    "<small>Descripción tiene la letra.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
